<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Estimation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Babak Shahbaba" />
    <script src="libs/header-attrs-2.14/header-attrs.js"></script>
    <link rel="stylesheet" href="slide-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide



&lt;br&gt;
&lt;br&gt;
.right-panel[ 
&lt;br&gt;

# Estimation

### Babak Shahbaba

]


---

## Parameter estimation

- We are interested in population mean and population variance, denoted as  `\(\mu\)` and `\(\sigma^{2}\)` respectively, of a
random variable. 

- These quantities are unknown in general. 

- We refer to these unknown quantities **parameters**.

- In the previous lecture, you learned about statistical methods for parameter **estimation**. 

- Estimation refers to the process of guessing the
unknown value of a parameter (e.g., population mean) using the observed
data. 


---

## Point estimation vs. interval estimation 


- Sometimes we only provide a single value as our estimate: sample mean for population mean and sample variance for population variance.

- This is called **point estimation**. 

- We use `\(\hat{\mu}\)` and `\(\hat{\sigma}^{2}\)` to denote the point estimates for `\(\mu\)` and `\(\sigma^{2}\)`. 

- Point estimates do not reflect our uncertainty.

- To address this issue, we can present our estimates in terms of a range of possible values (as opposed to a single value).

- This is called **interval estimation**.




---

## Estimating population mean


- Given `\(n\)` observed values, `\(X_{1}, X_{2}, \ldots, X_{n}\)`, from the
population, we can estimate the population mean `\(\mu\)` with the sample mean:
$$
`\begin{equation*}
\bar{X} = \frac{\sum_{i=1}^{n}X_{i}}{n}.
\end{equation*}`
$$

- In this case, we say that `\(\bar{X}\)` is an __estimator__ for `\(\mu\)`.

- The estimator itself is considered as a random variable since it value can change. 

---

## Estimating population mean

- We usually have only one sample of size `\(n\)` from the population `\(x_{1}, x_{2}, \ldots, x_{n}\)`. 

- Therefore, we only have one value for `\(\bar{X}\)`, which we denote `\(\bar{x}\)`:
$$
`\begin{equation*}
\bar{x} = \frac{\sum_{i=1}^{n}x_{i}}{n}
\end{equation*}`
$$

---

## Law of Large Numbers (LLN)

- The __Law of Large Numbers (LLN)__ indicates that (under some
general conditions such as independence of observations) the sample
mean converges to the population mean, `\(\bar{X}_{n} \to\mu\)`, as the sample size `\(n\)` increases, `\(n \to\infty\)`. 

- Informally, this means that the difference between the sample mean and the population mean tends to become smaller and smaller as we increase the sample size. 

- The Law of Large Numbers provides a theoretical
justification for the use of sample mean as an estimator for the
population mean.

- The Law of Large Numbers is true regardless of the underlying
distribution of the random variable. 


---

## Law of Large Numbers (LLN)

Suppose the true population mean for normal body temperature is 98.4F. 

&lt;img src="img/lln.png" width="40%" style="display: block; margin: auto;" /&gt;

---

## Law of Large Numbers (LLN)

- The Law of Large Numbers is true regardless of the underlying distribution of the random variable. 

- Therefore, it justifies using the sample mean `\(\bar{X}\)` not only to estimate the population mean for continuous random variables, but also for discrete random variables, whose values are counts and for discrete binary variables, whose possible values are 0 and 1 only. 

- For count variables, the mean is usually referred to as the __rate__ (e.g., rate of traffic accidents). 

- For binary random variables, the mean is usually referred to as the __proportion__ of the outcome of interest (denoted as 1). 


---

## Estimating population variance

- Given `\(n\)` randomly sampled values `\(X_{1}, X_{2}, \ldots, X_{n}\)` from
the population and their corresponding sample mean `\(\bar{X}\)`, we estimate the population variance as follows:
$$
`\begin{equation*}
S^{2}  =  {\frac{\sum_{i=1}^{n}(X_{i} - \bar{X})^{2}}{n-1}}.
\end{equation*}`
$$

- The sample standard deviation `\(S\)` (i.e., square root of
`\(S^2\)`) is our estimator of the population standard deviation `\({\sigma}\)`.



---

## Estimating population variance

- We regard the estimator `\(S^{2}\)` as a random variable.

- In practice, we usually have one set of observed values,
`\(x_{1}, x_{2}, \ldots, x_{n}\)`, and therefore, only one value for
`\(S^{2}\)`:

$$
`\begin{equation*}
s^{2}  =  \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})^{2}}{n-1}.
\end{equation*}`
$$

---

## Sampling distribution

- The value of estimators discussed so far (and all estimators in general) depend on the specific sample selected from the population.

- If we repeat our sampling, we are likely to obtain a different value for an estimator. 

- Therefore, we regard the estimators themselves as random variables. 

- As the result, similar to any other random variable, we can talk about their probability distribution. 

- Probability distributions for estimators are called __sampling distributions__. 



---

## Sampling distribution

- We focus on the sampling distribution of the sample mean `\(\bar{X}\)`.

- We start by assuming that the random variable of interest, `\(X\)`, has a
normal `\(N(\mu, \sigma^{2})\)` distribution. 

- Further, we assume that the population variance `\(\sigma^{2}\)` is known, so the only parameter we want to estimate is `\(\mu\)`.

- In this case:
`$$\begin{equation*}
\bar{X}  \sim N\bigl(\mu, \sigma^{2}/n\bigr).
\end{equation*}$$`
where `\(n\)` is the sample size. 


---

## Sampling distribution
&lt;center&gt;
&lt;img width="35%" src="img/distBP.png"/&gt;
&lt;img width="35%" src="img/sampleDistBP.png"/&gt;
&lt;/center&gt;
Left: The (unknown) theoretical distribution of blood pressure: `\(X\sim N(125, 15^2)\)`. Right: The density curve for the sampling distribution `\(\bar{X} \sim N(125, 15^{2}/100)\)` along with the histogram of 1000 sample means. 


---

## Confidence intervals for the population mean

- It is common to express our point estimate along with its standard
deviation to show how much the estimate could vary if different members
of population were selected as our sample. 

- Alternatively, we can use
the point estimate and its standard deviation to express our estimate
as a range (interval) of possible values for the unknown parameter.



---
## Confidence intervals for the population mean


- We know that `\(\bar{X} \sim N(\mu, \sigma^2/n)\)`. 

- Suppose that `\(\sigma^2 = 15^2\)` and sample size is `\(n=100\)`.

- Following the __68-95-99.7%__ rule, with 0.95 probability, the
value of `\(\bar{X}\)` is within 2 standard deviations from its mean, `\(\mu\)`,
`$$\begin{equation*}
\mu- 2\times1.5 \le\bar{X} \le\mu+ 2 \times1.5.
\end{equation*}$$`

- In other words, with probability 0.95,
`$$\begin{equation*}
\mu- 3 \le\bar{X} \le\mu+ 3.
\end{equation*}$$`

---

##Confidence intervals for the population mean

- We are, however, interested in estimating the population mean `\(\mu\)`
(instead of the sample mean `\(\bar{X}\)`). 

- By rearranging the terms of the
above inequality, we find that with
probability 0.95,
`$$\begin{equation*}
\bar{X} - 3 \le\mu\le\bar{X} + 3.
\end{equation*}$$`

- This means that with probability 0.95, the population mean `\(\mu\)` is in
the interval `\([\bar{X} - 3, \bar{X} + 3]\)`.


---

## Confidence intervals for the population mean}
- In reality, we usually have only one sample of `\(n\)`
observations, one sample mean `\(\bar{x}\)`, and one interval `\([\bar{x} - 3, \ \bar{x} + 3]\)` for the population mean `\(\mu\)`. 

- For the blood pressure example, suppose that
we have a sample of `\(n=100\)` people and that the sample mean is `\(\bar{x}=123\)`. Therefore, we have one interval as follows:
`$$\begin{equation*}
[123 - 3, 123 + 3] = [120, 126].
\end{equation*}$$`

- We refer to this interval as our 95% __confidence interval__ for the population mean `\(\mu\)`.

---

## Interpretation of confidence interval

&lt;img src="img/CI_SBP.png" width="40%" style="display: block; margin: auto;" /&gt;

---

## `\(z\)`-critical value

- In general, when the population variance `\(\sigma^{2}\)` is known, the 95% confidence interval for `\(\mu\)` is
obtained as follows:
`$$\begin{equation*}
\bigl[\bar{x} - 2 \times\sigma/ \sqrt{n},\ \bar{x} + 2 \times\sigma/\sqrt{n}\,\bigr]
\end{equation*}$$`

- We need a bigger interval if we want to be more confident, e.g., 99.7%:
`$$\begin{equation*}
\bigl[\bar{x} - 3 \times\sigma/ \sqrt{n},\ \bar{x} + 3 \times\sigma/\sqrt{n}\,\bigr]
\end{equation*}$$`

- Sometimes we can afford to be less confident, e.g., 68%:
`$$\begin{equation*}
\bigl[\bar{x} - 1 \times\sigma/ \sqrt{n},\ \bar{x} + 1 \times\sigma/\sqrt{n}\,\bigr]
\end{equation*}$$`


---

## `\(z\)`-critical value

- In general, the confidence interval for the population mean at
`\(c\)` (e.g., 0.8) confidence level is
`\begin{eqnarray*}
[\bar{x} - z_{crit}\times \sigma / \sqrt{n}, \ \bar{x}  + z_{crit} \times \sigma / \sqrt{n}]
\end{eqnarray*}`


---

## `\(z\)`-critical value
- We calculated `\(z_{crit}\)` by finding the `\((1+c)/2\)` quantile from the standard normal distribution. 

&lt;img src="img/zExplained.png" width="30%" style="display: block; margin: auto;" /&gt;



---

## Central limit theorem

- So far, we have assumed that the random variable has normal distribution, so the sampling distribution of `\(\bar{X}\)` is normal too.

- If the random variable is not normally distributed, the sampling distribution of `\(\bar{X}\)` can be considered as _approximately_ normal using (assuming the samples are independent and the sample size is large enough) the __central limit theorem (CLT)__:
_If the random variable `\(X\)` has the population mean `\(\mu\)` and the population variance `\(\sigma^{2}\)`, then the sampling distribution of `\(\bar{X}\)` is approximately normal with mean `\(\mu\)` and variance `\(\sigma^{2}/n\)`._

- Note that CLT is true regarding the underlying distribution of `\(X\)` so we can use it for random variables with Bernoulli and Binomial distributions too.


---

## Population Proportion


- For binary random variables, we are interested in estimating the population proportion, `\(\mu\)`. 

- For the corresponding Bernoulli distribution, the mean is `\(\mu\)` and the variance is `$$\sigma^2 = \mu(1-\mu)$$`

- Therefore, if estimate the `\(\mu\)`, we estimate both mean and variance (i.e., theoretical population mean and population variance). 


---

## Population Proportion

- Since estimating the population variance does not introduce an additional source of uncertainty to our analysis, so we do not need to use a `\(t\)`-distribution instead of the standard normal distribution.

- Given the sample proportion `\(p\)`, the confidence interval for the population proportion is obtained as follows:

`$$\begin{equation}
[p - z_{crit} \times\sqrt{p(1-p)/n}, \quad p + z_{crit} \times\sqrt{p(1-p)/n}]
\end{equation}$$`





---
## Confidence Interval When the Population Variance is Unknown

- So far, we have assumed the population variance, `\(\sigma^{2}\)`, of
the random variable is known or can be directly calculated using the mean for binary variables.

- However, we almost always need to estimate `\(\sigma^{2}\)`
along with the population mean `\(\mu\)`. 

- For this, we use the sample variance `\(s^{2}\)`.

- As a result, the standard deviation for `\(\bar{X}\)` is estimated to be `\(s/\sqrt{n}\)`. 


---

## Confidence Interval When the Population Variance is Unknown

- To find confidence intervals for the population mean when the population variance is unknown, we follow similar steps as described above, but
  - instead of `\(\sigma/\sqrt{n}\)` we use `\(s/\sqrt{n}\)`, 
  - instead of `\(z_{\mathrm{crit}}\)` based on the standard normal distribution, we use `\(t_{\mathrm{crit}}\)` obtained from a `\(t\)`-distribution with `\(n-1\)` degrees of freedom. 

-  The confidence interval for the population mean at
`\(c\)` confidence level is
`$$\begin{equation*}
\bigl[\bar{x} - t_{\mathrm{crit}}\times s / \sqrt{n}, \ \bar{x} + t_{\mathrm{crit}} \times s /
\sqrt{n}\,\bigr],
\end{equation*}$$`



---

## Margin of error

- We refer to `\(s/\sqrt{n}\)` as the __standard error__ of the sample mean `\(\bar{X}\)`.

- We can write the confidence interval as 
`$$\begin{equation*}
\bar{x} \pm t_{\mathrm{crit}}\times SE
\end{equation*}$$`


- The term `\(t_{\mathrm{crit}}\times SE\)` is called the
**margin of error** for the given confidence level. 


- It is common to present interval estimates for a given confidence level as
`$$\begin{equation*}
\textrm{Point estimate} \pm\textrm{Margin of error.}
\end{equation*}$$`

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "pygments",
"highlightLines": true,
"highlightLanguage": "r"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
