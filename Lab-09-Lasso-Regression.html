<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Lasso Regression Examples in R using the Alzheimerâ€™s Data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Brian Schetzsle" />
    <script src="libs/header-attrs-2.22/header-attrs.js"></script>
    <link rel="stylesheet" href="slide-style.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide



&lt;br&gt;
&lt;br&gt;
.right-panel[ 

# Lasso Regression Examples in R using the Alzheimer's Data
## Brian Schetzsle

]

---

class: middle

### Preparation

Load the `tidyverse` package into your environment and load your data into a variable. The code will look different and depends where you saved your data.



```r
library(tidyverse)
library(glmnet)
AD &lt;- readr::read_csv("./data/alzheimer_data.csv")
```

---

### Multicolinearity

The predictors in your model are usually correlated with each other (multicolinearity). For instance, `weight` tends to increase with `height`, i.e. taller people tend to be heavier. If you have a model that has both height and weight as predictors the multicolinearity between them will result in large variances of their fitted coefficients. This in turn inflates their p-values and the model may fail to find significance of either `height` or `weight`.

---

### Solutions to Multicolinearity

You could remove `height` and keep `weight`, which solves the problem of multicolinearity but means you lose out on any additional information about your response that `height` had but `weight` did not.

You could transform your data so your variables are no longer correlated (principal component analysis). This can be good because you don't throw out any information contained in your predictors but the results become difficult to interpret.

You could take information from less-important variables and shift it to more-important variables (lasso regression). This makes the fitted coefficients of the less-important variables go to 0 and it biases the more-important variables in a way that boosts the fit of the model.

---

### Linear Regression


```r
model1 &lt;- lm(naccicv ~ csfvol + lhippo + 
               rhippo + height + weight, 
             data = AD)
model1 %&gt;% summary()
```

```
## 
## Call:
## lm(formula = naccicv ~ csfvol + lhippo + rhippo + height + weight, 
##     data = AD)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -398.17  -54.08   -1.05   52.25  335.69 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 115.25595   29.50241   3.907 9.59e-05 ***
## csfvol        1.12335    0.02771  40.542  &lt; 2e-16 ***
## lhippo       38.65553    7.01351   5.512 3.89e-08 ***
## rhippo       66.75682    6.96919   9.579  &lt; 2e-16 ***
## height        8.23454    0.54133  15.212  &lt; 2e-16 ***
## weight        0.04409    0.05464   0.807     0.42    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 84.65 on 2694 degrees of freedom
## Multiple R-squared:  0.6063,	Adjusted R-squared:  0.6056 
## F-statistic: 829.9 on 5 and 2694 DF,  p-value: &lt; 2.2e-16
```

---

### Lasso Regression

The `glmnet` package wants the data for the model to be structures as a *data.matrix* rather than a *data.frame*. And the response needs to be a vector.


```r
X &lt;- AD %&gt;% 
  select(csfvol, lhippo, rhippo, height, weight) %&gt;% 
  data.matrix()

Y &lt;- AD$naccicv
```

---

### Lasso Regression


```r
#perform k-fold cross-validation to find optimal lambda value
cv_model &lt;- cv.glmnet(X, Y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda &lt;- cv_model$lambda.1se

#produce plot of test MSE by lambda value
plot(cv_model) 
```

&lt;img src="Lab-09-Lasso-Regression_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

### Lasso Regression



```r
#find coefficients of best model
model2 &lt;- glmnet(X, Y, alpha = 1, lambda = best_lambda)

#how do the fitted coefficients of the linear regression model
#compare to the coefficients in the lasso regression?
cbind(coef(model1), coef(model2))
```

```
## 6 x 2 sparse Matrix of class "dgCMatrix"
##                                   s0
## (Intercept) 115.25594602 321.2093767
## csfvol        1.12334758   0.9632218
## lhippo       38.65553059  24.1157277
## rhippo       66.75682464  59.0584354
## height        8.23453824   7.0888804
## weight        0.04409076   .
```

---

### Lasso Regression with Categorical Predictors


```r
X &lt;- AD %&gt;% 
  select(naccicv, csfvol, hallsev, 
         delsev, agitsev, depdsev) %&gt;% 
  mutate(hallsev = as.factor(hallsev),
         delsev = as.factor(delsev),
         agitsev = as.factor(agitsev),
         depdsev = as.factor(depdsev))

model3 &lt;- lm(naccicv ~ ., data = X)
model3 %&gt;% summary()
```

```
## 
## Call:
## lm(formula = naccicv ~ ., data = X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -379.71  -73.00   -4.76   68.00  387.35 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 927.68111   11.39308  81.425  &lt; 2e-16 ***
## csfvol        1.31394    0.03247  40.467  &lt; 2e-16 ***
## hallsev1    -31.69652   18.33698  -1.729  0.08400 .  
## hallsev2    -99.83784   34.40764  -2.902  0.00374 ** 
## hallsev3    -57.87952   40.86194  -1.416  0.15676    
## delsev1     -33.93340   14.03463  -2.418  0.01568 *  
## delsev2     -36.86368   19.18104  -1.922  0.05473 .  
## delsev3     -38.02397   23.83267  -1.595  0.11073    
## agitsev1    -31.26625    7.72733  -4.046 5.35e-05 ***
## agitsev2    -18.32691   10.11350  -1.812  0.07008 .  
## agitsev3      4.08040   19.83907   0.206  0.83706    
## depdsev1      6.83405    6.06204   1.127  0.25969    
## depdsev2    -12.71097    8.83694  -1.438  0.15044    
## depdsev3    -38.14982   18.76609  -2.033  0.04216 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 106.1 on 2686 degrees of freedom
## Multiple R-squared:  0.3836,	Adjusted R-squared:  0.3806 
## F-statistic: 128.6 on 13 and 2686 DF,  p-value: &lt; 2.2e-16
```

---

### Lasso Regression with Categorical Predictors

`glmnet` does like categorical predictors; I don't know why. You have to transform your data so that there is a new column for every level of every categorical predictor.


```r
X_matrix &lt;- model.matrix(~ ., X[,-1])[, -1]
Y = AD$naccicv
```

---

### Lasso Regression with Categorical Predictors


```r
#perform k-fold cross-validation to find optimal lambda value
cv_model &lt;- cv.glmnet(X_matrix, Y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda &lt;- cv_model$lambda.1se

#produce plot of test MSE by lambda value
plot(cv_model) 
```

&lt;img src="Lab-09-Lasso-Regression_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---

### Lasso Regression with Categorical Predictors


```r
#find coefficients of best model
model4 &lt;- glmnet(X_matrix, Y, alpha = 1, lambda = best_lambda)

#how do the fitted coefficients of the linear regression model
#compare to the coefficients in the lasso regression?
cbind(coef(model3), coef(model4))
```

```
## 14 x 2 sparse Matrix of class "dgCMatrix"
##                                s0
## (Intercept) 927.681110 957.935493
## csfvol        1.313943   1.214565
## hallsev1    -31.696521  -4.444562
## hallsev2    -99.837842 -34.551046
## hallsev3    -57.879515   .       
## delsev1     -33.933403 -12.229793
## delsev2     -36.863679  -5.836763
## delsev3     -38.023971  -6.684258
## agitsev1    -31.266246 -13.344773
## agitsev2    -18.326908  -6.588630
## agitsev3      4.080398   .       
## depdsev1      6.834054   .       
## depdsev2    -12.710975   .       
## depdsev3    -38.149818  -6.578887
```

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="cols_macro.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "pygments",
"highlightLines": true,
"highlightLanguage": "r"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
