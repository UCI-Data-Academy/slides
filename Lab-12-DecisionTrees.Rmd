---
title: "Decision Trees    \nExamples in R using the Alzheimer's Data"
author: "Brian Schetzsle"
output: 
  xaringan::moon_reader:
    css: ["slide-style.css", "https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"]
    lib_dir: libs
    seal: false
    nature:
      beforeInit: "cols_macro.js"
      ratio: 16:9
      highlightStyle: "pygments"
      highlightLines: true
      highlightLanguage: "r"

---

class: title-slide

```{r include=FALSE}
library(fabricerin)
```

<br>
<br>
.right-panel[ 

# `r rmarkdown::metadata$title`
## `r rmarkdown::metadata$author`

]

---

class: middle

### Preparation

Load the `tidyverse` package into your environment and load your data into a variable. The code will look different and depends where you saved your data. We will also use the `tidymodels` package to fit decision trees. Finally, the `rpart.plot` package allows us to visualize fitted tree models.


```{r message = FALSE, fontsize=1}
library(tidyverse)
library(tidymodels)
library(rpart.plot)
AD <- readr::read_csv("./data/alzheimer_data.csv")
```

---

### Decision Trees

Decision trees can be used for both classification and regression tasks. We will start with classification. We will return to the setup we had for the K Nearest Neighbors Algorithm Lab. We want to predict a person's sex (classification) as a function of their weight and height.

```{r eval=FALSE}
AD <- AD %>% 
  mutate(sex = ifelse(female == 0, "Male", "Female")) %>% 
  mutate(sex = as.factor(sex))

AD %>% 
  ggplot() +
  geom_point(aes(x = height, y = weight, color = sex))
```

---

### Clusters within data

```{r echo=FALSE, fig.align = "center", fig.width = 15}
AD <- AD %>% 
  mutate(sex = ifelse(female == 0, "Male", "Female")) %>% 
  mutate(sex = as.factor(sex))

AD %>% 
  ggplot() +
  geom_point(aes(x = height, y = weight, color = sex))
```

---

### Pre-processing and Splitting Data

Ordinarily you have to do some data pre-processing to deal with missing values. That issue has been graciously ameliorated by whomever cleaned this data for you from the much larger and messier NACC dataset. We do have to split our data into training and testing to avoid over-fitting. I'm using a 70-30 train-test split.

```{r}
n_total <- nrow(AD)
n_train <- floor(n_total * 0.7)
set.seed(123)
train_indices <- sample(1:n_total, n_train, 
                        replace = FALSE)

data_train <- AD %>% slice(train_indices)
data_test <- AD %>% slice(-train_indices)
```

---

### Fitting a Decision Tree

Using the `decision_tree()` function from the `tidymodels` package we can fit a decision tree.

```{r eval=FALSE}
#Create a decision tree model specification
tree_spec <- 
  decision_tree(mode = "classification", 
                engine = "rpart",
                cost_complexity = 0.005)
  

#Fit the model to the training data
tree_fit <- tree_spec %>% 
  fit(sex ~ height + weight, data = data_train)

rpart.plot(tree_fit$fit, roundint=FALSE)
```

---

### Fitting a Decision Tree

```{r echo=FALSE, fig.align = "center"}
#Create a decision tree model specification
tree_spec <- 
  decision_tree(mode = "classification", 
                engine = "rpart",
                cost_complexity = 0.005)
  

#Fit the model to the training data
tree_fit <- tree_spec %>% 
  fit(sex ~ height + weight, data = data_train)

rpart.plot(tree_fit$fit, roundint=FALSE)
```

---

### Fitting a Decision Tree

The decision tree made a single partition in the data. It classifies all those individuals taller than 67 as male and all those less than 67 as female.

```{r eval=FALSE}
data_train %>% 
  ggplot() +
  geom_point(aes(x = height, y = weight, color = sex)) +
  geom_vline(xintercept = 67)
```

---

### Fitting a Decision Tree

```{r echo=FALSE, fig.align = "center", fig.width = 15}
data_train %>% 
  ggplot() +
  geom_point(aes(x = height, y = weight, color = sex)) +
  geom_vline(xintercept = 67)
```

---

### Evaluating Accuracy

We now can make predictions on our test data using the fitted decision tree and compare the predicted sex to the actual sex.

```{r}
predictions <- predict(tree_fit, new_data = data_test) %>%
  pull(.pred_class)

confusion_matrix <- table(predictions, data_test$sex)

accuracy <- sum(diag(confusion_matrix)) / 
  sum(confusion_matrix)
```

---

### Evaluating Accuracy

```{r}
confusion_matrix

accuracy
```

---

### Evaluating accuracy

Where are these misclassifications happening?

```{r eval=FALSE}
cbind(data_test, predictions) %>% 
  mutate(correct = ifelse(predictions == sex, 
                          "correct", "incorrect")) %>% 
  ggplot() +
  geom_point(aes(x = height, y = weight, 
                 color = correct)) +
  scale_colour_manual(values = c("black", "red"))
```

---

### Evaluating accuracy

```{r echo=FALSE, fig.align = "center", fig.width = 15}
cbind(data_test, predictions) %>% 
  mutate(correct = ifelse(predictions == sex, 
                          "correct", "incorrect")) %>% 
  ggplot() +
  geom_point(aes(x = height, y = weight, 
                 color = correct)) +
  scale_colour_manual(values = c("black", "red"))
```

---

### Practice

* Fit a decision tree that classifies diagnosis (with three levels) as a function of any number of numeric and categorical predictors
* You may need to recode your categorical predictors using `as.factor()`
* Evaluate your accuracy
* Play around with the `cost_complexity` parameter in the `decision_tree()` function to see how it impacts accuracy

---

### Decision Trees for Regression

With classification we have a categorical response. We can also use tree-based models to predict a numeric response. The primary difference is the use of the `mode` parameter in the `decision_tree()` function being set to "regression" and not "classification". We will model age as a function of height and weight.

---

### Decision Trees for Regression

```{r eval=FALSE}
#Create a decision tree model specification
tree_spec <- 
  decision_tree(mode = "regression", 
                engine = "rpart",
                cost_complexity = 0.005)

#Fit the model to the training data
tree_fit <- tree_spec %>% 
  fit(age ~ height + weight, data = data_train)

rpart.plot(tree_fit$fit, roundint=FALSE)
```
---

### Decision Trees for Regression

```{r echo=FALSE, fig.align = "center", fig.width = 15}
#Create a decision tree model specification
tree_spec <- 
  decision_tree(mode = "regression", 
                engine = "rpart",
                cost_complexity = 0.005)

#Fit the model to the training data
tree_fit <- tree_spec %>% 
  fit(age ~ height + weight, data = data_train)

rpart.plot(tree_fit$fit, roundint=FALSE)
```

---

### Evaluating Accuracy

One way to evaluate how well your model is doing is with $R^2$, the amount of variance in our response (in the test subset) that is explained by your model. To do this we compare the variance of the residuals (difference between true and predicted values) to the variance of the response.

---

### Evaluating Accuracy

When getting predictions from a regression tree, the column we need to pull is `.pred` and not `.pred_class` as in decision trees.

```{r}
predictions <- predict(tree_fit, new_data = data_test) %>%
  pull(.pred)

R2 <- 1 - var(data_test$age - predictions) /
  var(data_test$age)
```

---

### Evaluating Accuracy

The $R^2$ value is really bad! It indicates that our regression tree does a terrible job modeling age; it is only able to explain about 4.5% of the variance of age. We can make our model more complicated by adjusting the `cost_complexity` parameter but run the risk of overfitting to our training data.

```{r}
R2
```

---

### Practice

* Select a numeric variable as your response and any number of numeric and categorical variables as your predictors
* Fit a regression tree to a training subset
* Evaluate how well the model fits the test data subset using $R^2$
* Adjust the `cost_complexity` parameter and see how this alters your $R^2$